{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "q5aXZTx677qt",
    "outputId": "1872d64f-aa47-4b8a-8e53-f6f854744faf"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from keras.datasets import cifar10\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N-5-9m7VMuO8"
   },
   "outputs": [],
   "source": [
    "(Xtrain, Ttrain), (Xtest, Ttest) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "yQ9KkprYLjEZ",
    "outputId": "a47e8171-70bc-4ad1-c16e-cf0b2f8ab1a3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 32, 32, 3), (10000, 32, 32, 3), (50000, 1), (10000, 1))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.shape, Xtest.shape, Ttrain.shape,Ttest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2on62BJVQ3HW"
   },
   "outputs": [],
   "source": [
    "Xtrain = Xtrain.reshape(-1, 3, 32, 32)\n",
    "Xtest = Xtest.reshape(-1, 3, 32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "KMKx1J-yROYS",
    "outputId": "0505f03e-164a-4b9c-dc69-1e97e3871e4c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 3, 32, 32), (10000, 3, 32, 32))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.shape, Xtest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CyX5TIMZ9iIv"
   },
   "outputs": [],
   "source": [
    "classes = ('Plane', 'Car', 'Bird', 'Cat',\n",
    "           'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XTWUW4ttBheU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "class CNN2D(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_inputs, n_hiddens_per_conv_layer, n_hiddens_per_fc_layer, n_outputs,\n",
    "                 patch_size_per_conv_layer, stride_per_conv_layer, activation_function='tanh', device='cpu'):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        n_conv_layers = len(n_hiddens_per_conv_layer)\n",
    "        if (len(patch_size_per_conv_layer) != n_conv_layers\n",
    "            or len(stride_per_conv_layer) != n_conv_layers):\n",
    "            raise Exception('The lengths of n_hiddens_per_conv_layer, patch_size_per_conv_layer, and stride_per_conv_layer must be equal.')\n",
    "\n",
    "        self.activation_function = torch.tanh if activation_function == 'tanh' else torch.relu\n",
    "\n",
    "        self.make_conv_and_fc_layers(n_inputs, n_hiddens_per_conv_layer, n_hiddens_per_fc_layer, n_outputs,\n",
    "                                     patch_size_per_conv_layer, stride_per_conv_layer)\n",
    "        \n",
    "        self.Xmeans = None\n",
    "        self.to(self.device)\n",
    "\n",
    "    def make_conv_and_fc_layers(self, n_inputs, n_hiddens_per_conv_layer, n_hiddens_per_fc_layer, n_outputs,\n",
    "                                patch_size_per_conv_layer, stride_per_conv_layer):\n",
    "                # Create all convolutional layers\n",
    "        # First argument to first Conv2d is number of channels for each pixel.\n",
    "        # Just 1 for our grayscale images.\n",
    "        n_in = 3\n",
    "        input_hw = int(np.sqrt(n_inputs))  # original input image height (=width because image assumed square)\n",
    "        self.conv_layers = torch.nn.ModuleList()\n",
    "        layeri = 0\n",
    "        for nh, patch_size, stride in zip(n_hiddens_per_conv_layer,\n",
    "                                          patch_size_per_conv_layer,\n",
    "                                          stride_per_conv_layer):\n",
    "            self.conv_layers.append(torch.nn.Conv2d(n_in, nh, kernel_size=patch_size, stride=stride))\n",
    "            conv_layer_output_hw = (input_hw - patch_size) // stride + 1\n",
    "            if conv_layer_output_hw <= 0:\n",
    "                raise Exception(f'''For conv layer {layeri}, input_hw of {input_hw} is less than patch_size {patch_size}.\n",
    "Try reducing the patch_size for this layer or for the previous layer.''')\n",
    "            input_hw = conv_layer_output_hw  # for next trip through this loop\n",
    "            n_in = nh\n",
    "            layeri += 1\n",
    "           \n",
    "        # Create all fully connected layers.  First must determine number of inputs to first\n",
    "        # fully-connected layer that results from flattening the images coming out of the last\n",
    "        # convolutional layer.\n",
    "        n_in = input_hw ** 2 * n_in\n",
    "        self.fc_layers = torch.nn.ModuleList()\n",
    "        for nh in n_hiddens_per_fc_layer:\n",
    "            self.fc_layers.append(torch.nn.Linear(n_in, nh))\n",
    "            n_in = nh\n",
    "        self.fc_layers.append(torch.nn.Linear(n_in, n_outputs))\n",
    "\n",
    "    def forward_all_outputs(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        Ys = [X]\n",
    "        for conv_layer in self.conv_layers:\n",
    "            Ys.append(self.activation_function(conv_layer(Ys[-1])))\n",
    "\n",
    "        flattened_input = Ys[-1].reshape(n_samples, -1)\n",
    "\n",
    "        for layeri, fc_layer in enumerate(self.fc_layers[:-1]):\n",
    "            if layeri == 0:\n",
    "                Ys.append(self.activation_function(fc_layer(flattened_input)))\n",
    "            else:\n",
    "                Ys.append(self.activation_function(fc_layer(Ys[-1])))\n",
    "\n",
    "        if len(self.fc_layers) == 1:\n",
    "            # only the output layer\n",
    "            Ys.append(self.fc_layers[-1](flattened_input))\n",
    "        else:\n",
    "            Ys.append(self.fc_layers[-1](Ys[-1]))\n",
    "\n",
    "        return Ys\n",
    "\n",
    "    def forward(self, X):\n",
    "        Ys = self.forward_all_outputs(X)\n",
    "        return Ys[-1]\n",
    "\n",
    "    def train(self, X, T, batch_size, n_epochs, learning_rate, method='sgd', verbose=True):\n",
    "        '''X and T must be numpy arrays'''\n",
    "\n",
    "        self.classes = np.unique(T)\n",
    "        T = np.arange(len(self.classes))[np.where(T.reshape(-1, 1) == self.classes)[1]]\n",
    "\n",
    "        # Set data matrices to torch.tensors\n",
    "        X = torch.from_numpy(X).float().to(self.device)\n",
    "        T = torch.from_numpy(T).long().to(self.device)  # required for classification in pytorch\n",
    "\n",
    "        # Setup standardization parameters\n",
    "        if self.Xmeans is None:\n",
    "            self.Xmeans = X.mean(axis=0)\n",
    "            self.Xstds = X.std(axis=0)\n",
    "            self.Xstds[self.Xstds == 0] = 1  # So we don't divide by zero when standardizing\n",
    "        #print(X.shape,self.Xmeans.shape)\n",
    "        # Standardize X\n",
    "        #print(X.shape,self.Xmeans.shape)\n",
    "        X = (X - self.Xmeans) / self.Xstds\n",
    "\n",
    "        X.requires_grad_(True)\n",
    "\n",
    "        if method == 'sgd':\n",
    "            optimizer = torch.optim.SGD(self.parameters(), lr=learning_rate, momentum=0.9)\n",
    "        else:\n",
    "            optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "        CELoss = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "        self.error_trace = []\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "\n",
    "            num_batches = X.shape[0] // batch_size\n",
    "            loss_sum = 0\n",
    "\n",
    "            for k in range(num_batches):\n",
    "                start = k * batch_size\n",
    "                end = (k + 1) * batch_size\n",
    "                X_batch = X[start:end, ...]\n",
    "                T_batch = T[start:end, ...]\n",
    "\n",
    "                Y = self.forward(X_batch)\n",
    "\n",
    "                loss = CELoss(Y, T_batch)\n",
    "                loss.backward()\n",
    "\n",
    "                # Update parameters\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                loss_sum += loss\n",
    "\n",
    "            self.error_trace.append(loss_sum / num_batches)\n",
    "\n",
    "            if verbose and (epoch + 1) % (max(1, n_epochs // 10)) == 0:\n",
    "                print(f'{method}: Epoch {epoch + 1} Loss {self.error_trace[-1]:.3f}')\n",
    "        #print(self.Xmeans.shape)\n",
    "        return self\n",
    "\n",
    "    def softmax(self, Y):\n",
    "        '''Apply to final layer weighted sum outputs'''\n",
    "        # Trick to avoid overflow\n",
    "        maxY = torch.max(Y, axis=1)[0].reshape((-1, 1))\n",
    "        expY = torch.exp(Y - maxY)\n",
    "        denom = torch.sum(expY, axis=1).reshape((-1, 1))\n",
    "        Y = expY / denom\n",
    "        return Y\n",
    "\n",
    "    def use(self, X):\n",
    "        # Set input matrix to torch.tensors\n",
    "        X = torch.from_numpy(X).float().to(self.device)\n",
    "        # Standardize X\n",
    "        #print(X.shape,self.Xmeans.shape)\n",
    "        X = (X - self.Xmeans) / self.Xstds\n",
    "        # Calculate output of net for all samples in X\n",
    "        Y = self.forward(X)\n",
    "        # Convert output to class probabilities\n",
    "        probs = self.softmax(Y)\n",
    "        # For each sample pick highest probability and translate that to class labels\n",
    "        classes = self.classes[torch.argmax(probs, axis=1).cpu().numpy()].reshape(-1, 1)\n",
    "        return classes, probs.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "eyYbcgipLaXv",
    "outputId": "5c307686-dcb1-4fc2-edb0-503e03056531"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Would you like to run on the GPU? (y or n): y\n",
      "Running on cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    y_or_n = input('Would you like to run on the GPU? (y or n): ')\n",
    "    if y_or_n == 'y' or y_or_n == 'yes':\n",
    "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running on', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 403
    },
    "colab_type": "code",
    "id": "D1sTuxGoOSeb",
    "outputId": "c2acfebd-29c9-48f3-ac96-44ae1ec6f871"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 2.00 GiB total capacity; 1.15 GiB already allocated; 281.00 MiB free; 1.15 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-b274fe866e21>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mcnnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_trace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Pytorch'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-5c0c232c7cdb>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X, T, batch_size, n_epochs, learning_rate, method, verbose)\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXmeans\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXmeans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXstds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXstds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXstds\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m  \u001b[1;31m# So we don't divide by zero when standardizing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;31m#print(X.shape,self.Xmeans.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 2.00 GiB total capacity; 1.15 GiB already allocated; 281.00 MiB free; 1.15 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "n_hiddens_per_conv_layer = [50, 50, 20]\n",
    "patch_size_per_conv_layer = [4, 2, 2]\n",
    "stride_per_conv_layer=[4, 4, 2]\n",
    "n_hiddens_per_fc_layer = [1000]\n",
    "\n",
    "cnnet = CNN2D(32 * 32, n_hiddens_per_conv_layer, n_hiddens_per_fc_layer, len(np.unique(Ttrain)), \n",
    "              patch_size_per_conv_layer, stride_per_conv_layer, activation_function='relu',device=device)\n",
    "\n",
    "n_epochs =2000\n",
    "batch_size = 300\n",
    "learning_rate = 0.001\n",
    "\n",
    "cnnet.train(Xtrain, Ttrain, batch_size, n_epochs, learning_rate, method='adam')\n",
    "\n",
    "plt.plot(cnnet.error_trace, label='Pytorch')\n",
    "plt.title('CIFAR-10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "fskb7JsLPNpR",
    "outputId": "2078881b-0ccd-4b6e-b26c-7d8fec4eef58"
   },
   "outputs": [],
   "source": [
    "Train_classes, Train_probs = cnnet.use(Xtrain)\n",
    "Test_classes, Test_probs = cnnet.use(Xtest)\n",
    "Train_percent = np.mean(Train_classes == Ttrain) * 100\n",
    "Test_percent = np.mean(Test_classes == Ttest) * 100\n",
    "print('Train Accuracy', Train_percent)\n",
    "print('Test Accuracy', Test_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BY595FNKVuGn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CIFAR-10_Project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
